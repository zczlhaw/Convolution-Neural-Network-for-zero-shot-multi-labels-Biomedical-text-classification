{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('../data/pubmed.csv')\n",
    "rm_labels = pd.read_csv('../data/rm_labels.csv', index_col=0,header = None, names=['Density'])\n",
    "rm_list = rm_labels.index.values\n",
    "\n",
    "def cleansing(text):\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def gen_wordidx(corpus):\n",
    "    word_idx = {}\n",
    "    index = 0\n",
    "    for i in range(len(corpus)):\n",
    "        temp = corpus[i]\n",
    "        word_idx[temp] = index\n",
    "        index += 1\n",
    "    word_idx['<pad>'] = i + 1\n",
    "    word_idx['<unknown>'] = i + 2\n",
    "    return word_idx\n",
    "\n",
    "def pad_sent(data):\n",
    "#     max_len = np.max([len(x) for x in data]) \n",
    "    max_len = 943\n",
    "    padded_data = []\n",
    "    for ab in data:\n",
    "        pads = ['<pad>'] * (max_len - len(ab))\n",
    "        padded_data.append(ab + pads)\n",
    "    return padded_data\n",
    "\n",
    "def con_idx(data, idx):\n",
    "    dat_idx = []\n",
    "    for ab in data:\n",
    "        temp_idx = []\n",
    "        for tok in ab:\n",
    "            if tok in idx:\n",
    "                temp_idx.append(idx[tok])\n",
    "            else:\n",
    "                temp_idx.append(idx['<unknown>'])\n",
    "        dat_idx.append(temp_idx)\n",
    "    return dat_idx\n",
    "\n",
    "#For initailed embedding\n",
    "def gen_corpus(data):\n",
    "    dct_word = {}\n",
    "    dct_word['<pad>'] = 0\n",
    "    dct_word['<unknown>'] = 1\n",
    "    index = 2\n",
    "    for i in range(len(data)):\n",
    "        temp = data[i]\n",
    "        for j in range(len(temp)):\n",
    "            if temp[j] not in dct_word:\n",
    "                dct_word[temp[j]] = index\n",
    "                index += 1\n",
    "    return dct_word\n",
    "\n",
    "def gen_vector(corpus, dim):\n",
    "    vector = []\n",
    "    vector.append(np.zeros([dim,]))\n",
    "    vector.append(np.zeros([dim,]))\n",
    "    for i in range(len(corpus)-2):\n",
    "        vector.append(np.random.uniform(-0.25, 0.25, [dim,]))\n",
    "    return vector\n",
    "\n",
    "dat['tokenize'] = dat['abstract']\n",
    "dat['tokenize'] = dat['tokenize'].fillna('None')\n",
    "dat['tokenize'] = dat['tokenize'].str.lower()\n",
    "dat['tokenize'] = dat['tokenize'].apply(cleansing)\n",
    "dat['tokenize'] = dat['tokenize'].str.split()\n",
    "\n",
    "x = list(dat['sequence'])\n",
    "\n",
    "yy = []\n",
    "for i in range(len(x)):\n",
    "    temp = x[i].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").split(\", \")\n",
    "    y = []\n",
    "    for j in range(len(temp)):\n",
    "        if j%2 != 0:\n",
    "            y.append(temp[j])\n",
    "    yy.append(y)\n",
    "    \n",
    "dat['mesh term code'] = yy\n",
    "\n",
    "temp = dat[['tokenize','mesh term code']].values\n",
    "\n",
    "rm_flag = np.zeros([np.shape(temp)[0],])\n",
    "for j in range(np.shape(temp)[0]):\n",
    "    for i in range(len(temp[j][1])):\n",
    "        if temp[j][1][i] in rm_list:\n",
    "            rm_flag[j] = 1\n",
    "            \n",
    "dat['rm_flag'] = rm_flag\n",
    "dat1 = dat[dat['rm_flag'] == 0]\n",
    "dat1 = dat1[['abstract', 'mesh terms', 'sequence', 'tokenize','mesh term code']]\n",
    "dat_unseen = dat[dat['rm_flag'] == 1]\n",
    "dat_unseen = dat_unseen[['abstract', 'mesh terms', 'sequence', 'tokenize','mesh term code']]\n",
    "\n",
    "dat_target = dat1['mesh term code']\n",
    "dat_target.to_csv('../data/target_prep_g.csv')\n",
    "\n",
    "temp_dat = dat1['tokenize']\n",
    "temp_dat = np.array(temp_dat)\n",
    "temp_dat = pad_sent(temp_dat)\n",
    "word_corpus = gen_corpus(temp_dat)\n",
    "temp_dat = con_idx(temp_dat, word_corpus)\n",
    "\n",
    "vec = gen_vector(word_corpus, 300)\n",
    "\n",
    "myFile = open('../data/vector_uni.csv', 'w')\n",
    "with myFile:\n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(vec)\n",
    "     \n",
    "print(\"Writing complete\")\n",
    "\n",
    "myFile = open('../data/input_prep_w2v_g.csv', 'w')\n",
    "with myFile:\n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(temp_dat)\n",
    "     \n",
    "print(\"Writing complete\")\n",
    "\n",
    "dat_unseen_target = dat_unseen['mesh term code']\n",
    "dat_unseen_target.to_csv('../data/target_prep_unseen_g.csv')\n",
    "\n",
    "temp_dat = dat_unseen['tokenize']\n",
    "temp_dat = np.array(temp_dat)\n",
    "temp_dat = pad_sent(temp_dat)\n",
    "temp_dat = con_idx(temp_dat, word_corpus)\n",
    "\n",
    "myFile = open('../data/input_prep_unseen_w2v_g.csv', 'w')\n",
    "with myFile:\n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(temp_dat)\n",
    "     \n",
    "print(\"Writing complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
