{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = open('../data/mesh_embeddings.txt').read().splitlines()\n",
    "\n",
    "idx_dw = []\n",
    "prep_dw = []\n",
    "for i in range(1,len(dw)):\n",
    "    l = dw[i].split()\n",
    "    idx_dw.append(int(l[0]))\n",
    "    prep_dw.append(l[1:])\n",
    "    \n",
    "with open('../data/index_2_code.pkl', 'rb') as f:\n",
    "    idx2cod = pickle.load(f)\n",
    "    \n",
    "with open('../data/code_2_index.pkl', 'rb') as f:\n",
    "    cod2idx = pickle.load(f)\n",
    "    \n",
    "dct_idx = {}\n",
    "for i in range(len(idx2cod)):\n",
    "    dct_idx[idx_dw[i]] = prep_dw[i]\n",
    "    \n",
    "for i in range(len(dct_idx)):\n",
    "    dct_idx[idx2cod[i]] = dct_idx.pop(i)\n",
    "    \n",
    "val_arr = []\n",
    "idx_list =[]\n",
    "for key, value in dct_idx.items():\n",
    "    temp = value#[key,value]\n",
    "    temp_idx = key\n",
    "    val_arr.append(temp)\n",
    "    idx_list.append(temp_idx)\n",
    "    \n",
    "idx_list = [idx.replace(':', '.') for idx in idx_list]\n",
    "sim = cosine_similarity(val_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Similarity Matrix for the first and second experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_target = pd.read_csv('../data/target_prep_g.csv', delimiter=\",\", index_col = 0,header = None, names=['mesh_term_code'])\n",
    "\n",
    "seen_dat = []\n",
    "for i in range(len(list(dat_target['mesh_term_code']))):\n",
    "    x = list(dat_target['mesh_term_code'])[i].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(\", \")\n",
    "    seen_dat.append(x)\n",
    "    \n",
    "mlb = MultiLabelBinarizer()\n",
    "dat_target = mlb.fit_transform(seen_dat)\n",
    "\n",
    "seen_classes = mlb.classes_\n",
    "\n",
    "dat = pd.read_csv('../data/pubmed.csv')\n",
    "\n",
    "x = list(dat['sequence'])\n",
    "\n",
    "yy = []\n",
    "for i in range(len(x)):\n",
    "    temp = x[i].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").split(\", \")\n",
    "    y = []\n",
    "    for j in range(len(temp)):\n",
    "        if j%2 != 0:\n",
    "            y.append(temp[j])\n",
    "    yy.append(y)\n",
    "    \n",
    "temp_mlb = MultiLabelBinarizer()\n",
    "temp = temp_mlb.fit_transform(yy)\n",
    "whole_classes = temp_mlb.classes_\n",
    "\n",
    "unseen_classes = []\n",
    "for i in range(len(whole_classes)):\n",
    "    if whole_classes[i] not in seen_classes:\n",
    "        unseen_classes.append(whole_classes[i])\n",
    "        \n",
    "dct_sim = {}\n",
    "sim_dct = {}\n",
    "for i in range(len(idx_list)):\n",
    "    dct_sim[idx_list[i]] = i\n",
    "    sim_dct[i] = idx_list[i]\n",
    "    \n",
    "seen_map = []\n",
    "for i in range(len(seen_classes)):\n",
    "    x = dct_sim[seen_classes[i]]\n",
    "    seen_map.append(x)\n",
    "        \n",
    "unseen_map = []\n",
    "for i in range(len(unseen_classes)):\n",
    "    x = dct_sim[unseen_classes[i]]\n",
    "    unseen_map.append(x)\n",
    "    \n",
    "temp1 = sim[np.reshape(seen_map,[-1]),]\n",
    "sim_mat = temp1[:,np.reshape(unseen_map,[-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Similarity Matrix for the third experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_target = pd.read_csv('../data/target_prep_g.csv', delimiter=\",\", index_col = 0,header = None, names=['mesh_term_code'])\n",
    "\n",
    "seen_dat = []\n",
    "for i in range(len(list(dat_target['mesh_term_code']))):\n",
    "    x = list(dat_target['mesh_term_code'])[i].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(\", \")\n",
    "    seen_dat.append(x)\n",
    "    \n",
    "mlb = MultiLabelBinarizer()\n",
    "dat_target = mlb.fit_transform(seen_dat)\n",
    "\n",
    "seen_classes = mlb.classes_\n",
    "\n",
    "dat = pd.read_csv('../data/pubmed.csv')\n",
    "\n",
    "x = list(dat['sequence'])\n",
    "\n",
    "yy = []\n",
    "for i in range(len(x)):\n",
    "    temp = x[i].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").split(\", \")\n",
    "    y = []\n",
    "    for j in range(len(temp)):\n",
    "        if j%2 != 0:\n",
    "            y.append(temp[j])\n",
    "    yy.append(y)\n",
    "    \n",
    "temp_mlb = MultiLabelBinarizer()\n",
    "temp = temp_mlb.fit_transform(yy)\n",
    "whole_classes = temp_mlb.classes_\n",
    "\n",
    "unseen_classes = []\n",
    "for i in range(len(whole_classes)):\n",
    "    if whole_classes[i] not in seen_classes:\n",
    "        unseen_classes.append(whole_classes[i])\n",
    "        \n",
    "dct_sim = {}\n",
    "sim_dct = {}\n",
    "for i in range(len(idx_list)):\n",
    "    dct_sim[idx_list[i]] = i\n",
    "    sim_dct[i] = idx_list[i]\n",
    "    \n",
    "top50 = ['A10.690.552.500', 'B01.050.150.900.649.801.400.112.400.400',\n",
    "        'B05.620.136', 'C14.907.253.855.600', 'C23.550.767',\n",
    "        'D09.546.359.448.500', 'D23.101.050', 'D23.101.140',\n",
    "        'D27.505.954.122.085', 'E01.370.600.550.324', 'E01.789', 'E02.186',\n",
    "        'E02.319.283', 'E02.319.283.199', 'E02.831.535.483',\n",
    "        'E05.598.500.374', 'E06.045', 'G01.910.857', 'G07.203.650.240.310',\n",
    "        'G09.330.380.076', 'G11.561.790.444', 'H01.770.644.728', 'I03.350',\n",
    "        'J02.500.456', 'L01.313.500.750.100.710.180', 'M01.060.116',\n",
    "        'M01.060.116.100.080', 'M01.060.116.630', 'M01.060.116.815',\n",
    "        'M01.060.703.520.520.500', 'M01.643.154', 'M01.643.364',\n",
    "        'M01.955.236', 'N02.421.726.407.680', 'N05.715.360.575.575.800',\n",
    "        'N06.850.505.200.100.175', 'N06.850.505.400.425.837',\n",
    "        'N06.850.520.308.980', 'N06.850.520.308.980.438.475.364.500',\n",
    "        'N06.850.520.308.985.525.375', 'N06.850.520.445.150',\n",
    "        'N06.850.520.445.300', 'N06.850.520.445.725', 'N06.850.520.445.850',\n",
    "        'N06.850.520.450.500.750.350', 'N06.850.520.450.500.750.650',\n",
    "        'N06.850.520.450.720', 'N06.850.520.830.600.800.725',\n",
    "        'N06.850.520.830.998.300', 'N06.850.810.250.180']\n",
    "\n",
    "seen_map = []\n",
    "for i in range(len(seen_classes)):\n",
    "    if seen_classes[i] in top50:\n",
    "        x = dct_sim[seen_classes[i]]\n",
    "        seen_map.append(x)\n",
    "\n",
    "unseen_map = []\n",
    "for i in range(len(unseen_classes)):\n",
    "    x = dct_sim[unseen_classes[i]]\n",
    "    unseen_map.append(x)\n",
    "\n",
    "for i in range(len(seen_classes)):\n",
    "    if seen_classes[i] not in top50:\n",
    "        x = dct_sim[seen_classes[i]]\n",
    "        unseen_map.append(x)\n",
    "        \n",
    "temp1 = sim[np.reshape(seen_map,[-1]),]\n",
    "sim_mat = temp1[:,np.reshape(unseen_map,[-1])]\n",
    "\n",
    "##Normailise top 10 similarity between seen labels and unseen labels\n",
    "# for i in range(50):\n",
    "#     sim_mat[:,i][np.argsort(np.argsort(sim_mat[:,i])) < 40] = 0\n",
    "#     sim_mat[:,i] = (sim_mat[:,i] - np.min(sim_mat[:,i]))/(np.max(sim_mat[:,i])-np.min(sim_mat[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Similarity Matrix for the forth experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_target = pd.read_csv('../data/target_prep_g.csv', delimiter=\",\", index_col = 0,header = None, names=['mesh_term_code'])\n",
    "\n",
    "seen_dat = []\n",
    "for i in range(len(list(dat_target['mesh_term_code']))):\n",
    "    x = list(dat_target['mesh_term_code'])[i].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(\", \")\n",
    "    seen_dat.append(x)\n",
    "    \n",
    "mlb = MultiLabelBinarizer()\n",
    "dat_target = mlb.fit_transform(seen_dat)\n",
    "\n",
    "seen_classes = mlb.classes_\n",
    "\n",
    "dat = pd.read_csv('../data/pubmed.csv')\n",
    "\n",
    "x = list(dat['sequence'])\n",
    "\n",
    "yy = []\n",
    "for i in range(len(x)):\n",
    "    temp = x[i].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").split(\", \")\n",
    "    y = []\n",
    "    for j in range(len(temp)):\n",
    "        if j%2 != 0:\n",
    "            y.append(temp[j])\n",
    "    yy.append(y)\n",
    "    \n",
    "temp_mlb = MultiLabelBinarizer()\n",
    "temp = temp_mlb.fit_transform(yy)\n",
    "whole_classes = temp_mlb.classes_\n",
    "\n",
    "unseen_classes = []\n",
    "for i in range(len(whole_classes)):\n",
    "    if whole_classes[i] not in seen_classes:\n",
    "        unseen_classes.append(whole_classes[i])\n",
    "        \n",
    "dct_sim = {}\n",
    "sim_dct = {}\n",
    "for i in range(len(idx_list)):\n",
    "    dct_sim[idx_list[i]] = i\n",
    "    sim_dct[i] = idx_list[i]\n",
    "    \n",
    "top50 = ['A10.690.552.500', 'B01.050.150.900.649.801.400.112.400.400',\n",
    "        'B05.620.136', 'C14.907.253.855.600', 'C23.550.767',\n",
    "        'D09.546.359.448.500', 'D23.101.050', 'D23.101.140',\n",
    "        'D27.505.954.122.085', 'E01.370.600.550.324', 'E01.789', 'E02.186',\n",
    "        'E02.319.283', 'E02.319.283.199', 'E02.831.535.483',\n",
    "        'E05.598.500.374', 'E06.045', 'G01.910.857', 'G07.203.650.240.310',\n",
    "        'G09.330.380.076', 'G11.561.790.444', 'H01.770.644.728', 'I03.350',\n",
    "        'J02.500.456', 'L01.313.500.750.100.710.180', 'M01.060.116',\n",
    "        'M01.060.116.100.080', 'M01.060.116.630', 'M01.060.116.815',\n",
    "        'M01.060.703.520.520.500', 'M01.643.154', 'M01.643.364',\n",
    "        'M01.955.236', 'N02.421.726.407.680', 'N05.715.360.575.575.800',\n",
    "        'N06.850.505.200.100.175', 'N06.850.505.400.425.837',\n",
    "        'N06.850.520.308.980', 'N06.850.520.308.980.438.475.364.500',\n",
    "        'N06.850.520.308.985.525.375', 'N06.850.520.445.150',\n",
    "        'N06.850.520.445.300', 'N06.850.520.445.725', 'N06.850.520.445.850',\n",
    "        'N06.850.520.450.500.750.350', 'N06.850.520.450.500.750.650',\n",
    "        'N06.850.520.450.720', 'N06.850.520.830.600.800.725',\n",
    "        'N06.850.520.830.998.300', 'N06.850.810.250.180']\n",
    "\n",
    "unseen50 = ['A01.378.610.450', 'C05.799.613.750', 'C14', 'C14.907.489.631.485',\n",
    "       'C17.800.090.500.260', 'C19.391.630.705.331', 'C20.673.480',\n",
    "       'C23.550.291.656', 'D12.644.548.586.200.500.625.700',\n",
    "       'D27.505.696.422', 'D27.505.954.329.030',\n",
    "       'D27.505.954.427.210.100.200', 'D27.505.954.427.210.600.500',\n",
    "       'D27.888.569.035.035', 'E02.319.267.510.795',\n",
    "       'E02.319.267.530.620.570', 'E02.319.310.037', 'E04.502.515',\n",
    "       'E05.333.250', 'E05.629.937.260.850', 'E05.642.249.500',\n",
    "       'F02.463.188', 'F02.830.104.214', 'F04.586', 'F04.754.137.428',\n",
    "       'G07.100.049', 'G07.100.100.160.120.699.500.500', 'G07.690.773.750',\n",
    "       'G08.686.784.769.580', 'G09.330.380', 'G09.330.380.500.430',\n",
    "       'G11.427.560', 'G16.757', 'I03.450.642.845.940',\n",
    "       'L01.224.230.110.500', 'N02.421.585.400.480', 'N04.590.233.727',\n",
    "       'N05.300.150.410', 'N05.300.150.600.600', 'N05.715.360.575.575',\n",
    "       'N05.715.360.600', 'N05.715.360.775.225', 'N06.850.490.250',\n",
    "       'N06.850.520.308.985.550.900', 'N06.850.520.445.800.650',\n",
    "       'N06.850.520.450.500.750.825', 'N06.850.520.830.150',\n",
    "       'N06.850.520.830.600.800.715', 'N06.850.520.830.750.725',\n",
    "       'N06.850.520.830.998.650', 'Z01.107.567.875']\n",
    "\n",
    "seen_map = []\n",
    "for i in range(len(seen_classes)):\n",
    "    if seen_classes[i] in top50:\n",
    "        x = dct_sim[seen_classes[i]]\n",
    "        seen_map.append(x)\n",
    "\n",
    "unseen_map = []\n",
    "for i in unseen50:\n",
    "    x = dct_sim[i]\n",
    "    unseen_map.append(x)\n",
    "        \n",
    "temp1 = sim[np.reshape(seen_map,[-1]),]\n",
    "sim_mat = temp1[:,np.reshape(unseen_map,[-1])]\n",
    "\n",
    "##Normailise top 10 similarity between seen labels and unseen labels\n",
    "# for i in range(50):\n",
    "#     sim_mat[:,i][np.argsort(np.argsort(sim_mat[:,i])) < 40] = 0\n",
    "#     sim_mat[:,i] = (sim_mat[:,i] - np.min(sim_mat[:,i]))/(np.max(sim_mat[:,i])-np.min(sim_mat[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing output file \"sim_mat.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFile = open('../data/sim_mat.csv', 'w')\n",
    "with myFile:\n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(sim_mat)\n",
    "     \n",
    "print(\"Writing complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
