{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "dat_seen_input = np.array(pd.read_csv('../data/input_prep_w2v_g.csv',sep = ',',header = None))\n",
    "\n",
    "temp_dat_target = pd.read_csv('../data/target_prep_g.csv', delimiter=\",\", index_col = 0,header = None, names=['mesh_term_code'])\n",
    "temp_seen_dat = []\n",
    "    \n",
    "for i in range(len(list(temp_dat_target['mesh_term_code']))):\n",
    "    x = list(temp_dat_target['mesh_term_code'])[i].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(\", \")\n",
    "    temp_seen_dat.append(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlb_temp = MultiLabelBinarizer()\n",
    "dat_seen_target = mlb_temp.fit_transform(temp_seen_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_seen = 1/(np.sum(dat_seen_target,0)/len(temp_seen_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_unseen_input = np.array(pd.read_csv('../data/input_prep_unseen_w2v_g.csv',sep = ',',header = None))\n",
    "\n",
    "temp_dat_unseen_target = pd.read_csv('../data/target_prep_unseen_g.csv', delimiter=\",\", index_col = 0,header = None, names=['mesh_term_code'])\n",
    "unseen_dat = []\n",
    "for i in range(len(list(temp_dat_unseen_target['mesh_term_code']))):\n",
    "    x = list(temp_dat_unseen_target['mesh_term_code'])[i].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(\", \")\n",
    "    unseen_dat.append(x)\n",
    "    \n",
    "final_unseen = []\n",
    "final_seen = []\n",
    "for i in range(len(unseen_dat)):\n",
    "    x = []\n",
    "    xx = []\n",
    "    for j in range(len(unseen_dat[i])):\n",
    "        if unseen_dat[i][j] not in mlb_temp.classes_:\n",
    "            x.append(unseen_dat[i][j])\n",
    "        else:\n",
    "            xx.append(unseen_dat[i][j])\n",
    "    final_unseen.append(x)\n",
    "    final_seen.append(xx)\n",
    "    \n",
    "mlb_unseen = MultiLabelBinarizer()\n",
    "dat_unseen_target = mlb_unseen.fit_transform(final_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim = np.array(pd.read_csv('../data/sim_mat.csv',sep = ',',header = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = np.array(pd.read_csv('../data/vector_glove.csv',sep = ',',header = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dat_seen_input, dat_seen_target,\n",
    "                                                    test_size = 0.20, random_state = 3)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                    test_size = 0.20, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 943\n",
    "num_filter = 100\n",
    "dim_emb = 300\n",
    "learning_rate = 0.01\n",
    "\n",
    "num_seen_outs = len(mlb_temp.classes_)\n",
    "num_unseen_outs = len(mlb_unseen.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, seq_len], name=\"X\")\n",
    "y_ = tf.placeholder(tf.float32, [None, num_seen_outs], name=\"Y\")\n",
    "y_unseen_ = tf.placeholder(tf.float32, [None, num_unseen_outs], name=\"Y_unseen\")\n",
    "prob_drop = tf.placeholder(tf.float32, None, name=\"dropout_prob\")\n",
    "dec_lvl = tf.placeholder(tf.float32, None, name=\"decision_boundary\")\n",
    "\n",
    "sim_seen2unseen = tf.placeholder(tf.float32,[num_seen_outs, num_unseen_outs], name='sim_matrix')\n",
    "\n",
    "embeddings = tf.placeholder(tf.float32, [None, dim_emb], name='word2vec')\n",
    "\n",
    "embedded_dat = tf.nn.embedding_lookup(embeddings, X)\n",
    "\n",
    "with tf.name_scope(\"fmodel\"):\n",
    "    conv_3 = tf.layers.conv1d(inputs=embedded_dat, filters=num_filter, kernel_size=3,\n",
    "                              padding=\"VALID\", strides=1, activation = tf.nn.relu,\n",
    "                             kernel_initializer= tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    pooled_3 = tf.layers.max_pooling1d(conv_3, pool_size = 941, strides = 1)\n",
    "    \n",
    "    conv_4 = tf.layers.conv1d(inputs=embedded_dat, filters=num_filter, kernel_size=4,\n",
    "                              padding=\"VALID\", strides=1, activation = tf.nn.relu,\n",
    "                             kernel_initializer= tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    pooled_4 = tf.layers.max_pooling1d(conv_4, pool_size = 940, strides = 1)\n",
    "    \n",
    "    conv_5 = tf.layers.conv1d(inputs=embedded_dat, filters=num_filter, kernel_size=5,\n",
    "                              padding=\"VALID\", strides=1, activation = tf.nn.relu,\n",
    "                             kernel_initializer= tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    pooled_5 = tf.layers.max_pooling1d(conv_5, pool_size = 939, strides = 1)\n",
    "    \n",
    "    pooled_outputs = tf.concat([pooled_3, pooled_4, pooled_5],1)\n",
    "    flat_layer = tf.reshape(pooled_outputs, [-1, 3 * num_filter])\n",
    "    \n",
    "    drop_out = tf.nn.dropout(flat_layer, prob_drop)    \n",
    "    \n",
    "    y_drop = tf.layers.dense(inputs=drop_out, units=num_seen_outs, name = 'drop_out')\n",
    "    yy_drop = tf.nn.sigmoid(y_drop)\n",
    "    \n",
    "    \n",
    "    unseen_dense = np.divide(tf.matmul(y_drop, sim_seen2unseen), num_seen_outs)\n",
    "    sig_unseen_dense = tf.nn.sigmoid(unseen_dense)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y_, logits = y_drop))\n",
    "#     loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets = y_, logits = y_drop, \n",
    "#                                                                    pos_weight = weight_seen))\n",
    "    \n",
    "    opt_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    \n",
    "    fn = tf.metrics.false_negatives(labels=y_, predictions=tf.greater(yy_drop, dec_lvl))\n",
    "    fp = tf.metrics.false_positives(labels=y_, predictions=tf.greater(yy_drop, dec_lvl))\n",
    "    tp = tf.metrics.true_positives(labels=y_, predictions=tf.greater(yy_drop, dec_lvl))\n",
    "    \n",
    "    fn_unseen = tf.metrics.false_negatives(labels=y_unseen_, predictions=tf.greater(sig_unseen_dense, dec_lvl))\n",
    "    fp_unseen = tf.metrics.false_positives(labels=y_unseen_, predictions=tf.greater(sig_unseen_dense, dec_lvl))\n",
    "    tp_unseen = tf.metrics.true_positives(labels=y_unseen_, predictions=tf.greater(sig_unseen_dense, dec_lvl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    sess.run(init)\n",
    "    sess.run(init_l) \n",
    "    cur_f1 = 0\n",
    "    cnt = 0\n",
    "    epoch = 0\n",
    "    \n",
    "    while (cnt < 4 and epoch < 14):\n",
    "        \n",
    "        print('----- Epoch', epoch, '-----')\n",
    "        for batch in range(len(X_train)//batch_size):\n",
    "\n",
    "            batch_x = np.reshape(X_train[batch*batch_size:min((batch+1)*batch_size,len(X_train))], [-1, seq_len])\n",
    "            batch_y = np.reshape(y_train[batch*batch_size:min((batch+1)*batch_size,len(y_train))], [-1, num_seen_outs])\n",
    "\n",
    "            opt = sess.run(opt_op, feed_dict={X: batch_x, y_: batch_y, embeddings : emb, prob_drop : 0.5})\n",
    "            loss_ = sess.run(loss, feed_dict={X: batch_x, y_: batch_y, embeddings : emb, prob_drop : 0.5})\n",
    "            print(loss_)\n",
    "\n",
    "        print('----- Validation', '-----')\n",
    "        log_p = []\n",
    "        log_r = []\n",
    "        for batch in range(len(X_val)//512):\n",
    "            batch_val_x = np.reshape(X_val[batch*batch_size:min((batch+1)*batch_size,len(X_val))], [-1, seq_len])\n",
    "            batch_val_y = np.reshape(y_val[batch*batch_size:min((batch+1)*batch_size,len(y_val))], [-1, num_seen_outs])\n",
    "                        \n",
    "            fn_t, fp_t, tp_t = sess.run([fn, fp, tp], feed_dict={X: batch_val_x, \n",
    "                                              y_: batch_val_y, embeddings : emb, prob_drop : 0.5, dec_lvl: 0.5})\n",
    "            p = tp_t[0]/(tp_t[0]+fp_t[0])\n",
    "            r = tp_t[0]/(tp_t[0]+fn_t[0])\n",
    "\n",
    "            log_p.append(p)\n",
    "            log_r.append(r)\n",
    "\n",
    "        print('tf_precision_recall_f1score:',np.mean(log_p), np.mean(log_r), round(np.mean(log_p) * np.mean(log_r) * 2/(np.mean(log_p) + np.mean(log_r)),4))\n",
    "        if round(np.mean(log_p) * np.mean(log_r) * 2/(np.mean(log_p) + np.mean(log_r)),4) > cur_f1:\n",
    "            save_path = saver.save(sess, \"../model/glove/model.ckpt\")\n",
    "            print('Save Model Success')\n",
    "            cur_f1 = round(np.mean(log_p) * np.mean(log_r) * 2/(np.mean(log_p) + np.mean(log_r)),4)\n",
    "            cnt = 0\n",
    "        else:\n",
    "            cnt += 1\n",
    "        epoch += 1\n",
    "print(\"End Training Session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    result = []\n",
    "    target = []\n",
    "    log_p = []\n",
    "    log_r = []\n",
    "    temptemp = []\n",
    "    sess.run(init_l)\n",
    "    \n",
    "    saver.restore(sess, \"../model/glove/model.ckpt\")\n",
    "    \n",
    "    for batch in range(len(X_val)//512):\n",
    "        batch_val_x = np.reshape(X_val[batch*batch_size:min((batch+1)*batch_size,len(X_val))], [-1, seq_len])\n",
    "        batch_val_y = np.reshape(y_val[batch*batch_size:min((batch+1)*batch_size,len(y_val))], [-1, num_seen_outs])\n",
    "                        \n",
    "        fn_t, fp_t, tp_t = sess.run([fn, fp, tp], feed_dict={X: batch_val_x, \n",
    "                                              y_: batch_val_y, embeddings : emb, prob_drop : 0.5, dec_lvl: 0.8})\n",
    "        p = tp_t[0]/(tp_t[0]+fp_t[0])\n",
    "        r = tp_t[0]/(tp_t[0]+fn_t[0])\n",
    "\n",
    "        log_p.append(p)\n",
    "        log_r.append(r)\n",
    "\n",
    "    print('tf_precision_recall_f1score:',np.mean(log_p[1:]), np.mean(log_r[1:]), \n",
    "          round(np.mean(log_p[1:]) * np.mean(log_r[1:]) * 2/(np.mean(log_p[1:]) + \n",
    "                                                             np.mean(log_r[1:])),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    result = []\n",
    "    target = []\n",
    "    log_p = []\n",
    "    log_r = []\n",
    "    temptemp = []\n",
    "    sess.run(init_l)\n",
    "    \n",
    "    saver.restore(sess,\"../model/original_weight/model.ckpt\")\n",
    "    \n",
    "    for batch in range(len(dat_unseen_input)//batch_size):\n",
    "        batch_val_x = np.reshape(dat_unseen_input[batch*batch_size:min((batch+1)*batch_size,len(dat_unseen_input))], [-1, seq_len])\n",
    "        batch_val_y = np.reshape(dat_unseen_target[batch*batch_size:min((batch+1)*batch_size,len(dat_unseen_target))], [-1, num_unseen_outs])\n",
    "\n",
    "        fn_t, fp_t,  tp_t = sess.run([fn_unseen, fp_unseen, tp_unseen], feed_dict={X: batch_val_x, \n",
    "                                          y_unseen_: batch_val_y, embeddings : emb, prob_drop : 0.5,\n",
    "                                                             dec_lvl: 0.45, sim_seen2unseen: sim})\n",
    "        p = tp_t[0]/(tp_t[0]+fp_t[0])\n",
    "        r = tp_t[0]/(tp_t[0]+fn_t[0])\n",
    "\n",
    "        log_p.append(p)\n",
    "        log_r.append(r)\n",
    "        \n",
    "    print('tf_precision_recall_f1score:',np.mean(log_p[1:]), np.mean(log_r[1:]), round(np.mean(log_p[1:]) * np.mean(log_r[1:]) * 2/(np.mean(log_p[1:]) + np.mean(log_r[1:])),4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
